{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Mining (JADS) - Assignment 4\n",
    "This assignment is about SVMs, Neural nets and preprocessing. We will again use several new datasets from  https://www.openml.org. This time you will also use OpenML to upload your own experiments.\n",
    "\n",
    "The same rules as the previous assignments regarding report length and formatting apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 1: Support Vector Machines and scaling (1 point)\n",
    "Evaluate the performance of a Support Vector Machine on the Seismic Bumps dataset, using 10-fold cross-validation. More information on the dataset here: https://www.openml.org/d/1500. Download with the download icon. If the downloaded file does not have an .arff extension, simply add that extension.\n",
    "\n",
    "* In WEKA Explorer, load the dataset and look at the distribution of the attribute values. Select a few attributes (e.g. attribute 1,2,3) both in the list (left) as the visualization target (in the lower right bar chart). Are the values normally distributed?\n",
    "* Now, under filters, find the 'Standardize' filter and apply it to your data. Describe the effect on the distribution of your data. Reload the data, and now try the 'Normalize' filter instead. What is the difference? \n",
    "* Reload the data, and now apply an SVM classfier (called functions.SMO in WEKA). First, build and evaluate an SVM model (with default hyperparameters) and report the performance (accuracy, i.e. percent correct).\n",
    "* Now, in the hyperparameter settings, switch off the internal normalization by setting `filterType' to 'No normalization/standardization'. What is the effect on performance? Does it matter whether you choose the normalize or standardize filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Solution **\n",
    "\n",
    "* No, they are not normally distributed. For instance, attribute 1 and 2:\n",
    "![Dist](./images/A4_1a.png)\n",
    "Some attributes (like V3) are close to normal, but not really.\n",
    "* Standardisation shows the same distribution, but now centered around mean 0 with standard deviation 1. Normalization also maintains the same distribution, but shifted and normalized to interval [0,1]:\n",
    "![Dist](./images/A4_1b.png)\n",
    "* SVM yields an accuracy of 93.8%.\n",
    "* Without normalization/standardization, the accuracy is only 92.38%. There is no difference between choosing normalization or standardization on this data. The difference is typically small anyway, as long as the attribute values are on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 2: Feature Selection (2 points)\n",
    "Study the effect of feature selection on the Arrythmia dataset https://www.openml.org/d/5. It measures heart arrythmia based on sensor measurements, but not all sensors are equally useful.\n",
    "\n",
    "* Load the dataset and select the most relevant features using correlation-based feature selection (CfsSubsetEval). Do this in the 'Select Attributes' tab. How many of the 280 features are selected?\n",
    "* Build a normal k-Nearest Neighbor (kNN) classifier (use k=3) and report the performance (accuracy).\n",
    "* Now perform feature selection and then run the same nearest neighbor classifier. This can be done with the AttributeSelectedClassfier. Use the standard evaluator (CfsSubsetEval), but be sure to set kNN as the classifier. Again, report the performance. Give a clear explanation why the performance is different.\n",
    "* Repeat the above, but now with a decision tree (J48) instead of kNN. Do you still see a performance difference? Why (not)?\n",
    "* Finally, replace the evaluator with the 'InfoGainAttributeEval' (also select 'Ranker' under Search), and compare its performance to J48 without feature selection. Does this yield different performances? Explain why (not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Solution **\n",
    "\n",
    "* CfsSubsetEval selects 26 features\n",
    "![AttrSel](./images/A4_1.png)\n",
    "* 3-NN accuracy: 57.74%\n",
    "* 'Attribute Selection + kNN' accuracy: 65.92%. kNN works best if all features are important because of the curse of dimensionality. Distances between points that include many irrelevant dimensions are less meaningful, and lead to worse predictions.\n",
    "* 'Attribute Selection + J48' accuracy: 68.1416%, J48 accuracy: 64.3805%. The performance different is smaller because decision trees inherently select the best features first. Still, with many irrelevant features, decision trees will overfit because deeper in the tree, where few data points remain in each leaf, it becomes more likely that random features correlate with class labels just by chance, and hence the tree starts overfitting because of lack of data.\n",
    "* 'Attribute Selection based on InfoGain + J48' accuracy: 64.1593%. Almost no performance difference with normal J48 because it uses the same heuristic, and hence has the same problem as described above. The default CfsSubsetEval heuristic uses correlation instead of information gain, and hence is a better choice to use with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 3: PCA (1 point)\n",
    "Study the effect of PCA on the Glass dataset https://www.openml.org/d/41.\n",
    "\n",
    "* Load the dataset, and visualize the first two attributes in the Visualize tab. Do the first two features neatly separate the classes (colors)? Include a screenshot and discuss.\n",
    "* Run PCA (the Principal Component filter) on your data, and Visualize the first two principle components (those are the first two attributes after applying PCA). Is the data better separated? Include a screenshot and discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Solution **\n",
    "\n",
    "* First 2 features: \n",
    "![no pca](./images/A4_2A.png)\n",
    "* First 2 principal components: \n",
    "![pca](./images/A4_2B.png)\n",
    "The data is separated better, but only slightly. The yellow points ('headlamps') are more cleanly seperated out, as well as the purple ('tableware') and pink ('containers') points. However, they were already somewhat seperated in the original two dimensions. Indeed, the goal of PCA is to reduce dimensions while preserving pariwise distances, hence it doesn't directly separate the classes. It is, in fact, an unsupervised method so there is no specific class attribute taken into account. Still, it does also help separate classes if the points of these classes are also close together in the original (high dimensional) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: PCA and kNN (1 point)\n",
    "Study the effect of PCA on the performance of kNN on the Isolet dataset https://www.openml.org/d/300.\n",
    "\n",
    "* Load the dataset, and train a normal kNN classifier (use k=3). Report the performance (accuracy).\n",
    "* Run PCA (the Principal Component filter) on your data, and set the number of retured attributes to 40.\n",
    "Now, build the same kNN classifier on the 40 principal component. Report the difference in performance and explain. Does the PCA manage to retain most of the information in 40 of the 618 features?\n",
    "\n",
    "Note: building these models may take a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Solution **\n",
    "\n",
    "* 3-NN has an accuracy of 86.47%\n",
    "* PCA-40 yields an accuracy of 85.03%. The performance different is very small, hence PCA retains the information well. Note that the dataset is not very high dimensional, and thus kNN without PCA works quite well. For more high-dimensional datasets, PCA would likely improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Kernel Selection (2 points)\n",
    "Study the effect of different kernels in SVMs on the EEG Eye State dataset https://www.openml.org/d/1471.\n",
    "    \n",
    "* Build 3 models, using the default Linear (Polynomial with degree 1), Polynomial (degree 2), and RBF kernel. Report the performances.\n",
    "* For the Polynomial kernel, change the degree to values [2,3,4,5,10,50]. Report the performances (e.g. use a table or a line plot) and discuss. When do you think the SVM is under/overfitting?\n",
    "* For the RBF kernel, change the gamma parameter to values [0.001,0.01,0.1,1,10,100,1000]. Again, report the performances (e.g. use a table or a line plot) and discuss. When do you think the SVM is under/overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Solution **\n",
    "\n",
    "* Because training the SVM takes a long time, we take a 25% stratisfied subsample. Results:\n",
    "\n",
    "Kernel | Accuracy\n",
    "--- | ---\n",
    "Linear | 55.13\n",
    "Poly (degree 2) | 60.28\n",
    "RBF | 55.12\n",
    "\n",
    "The data is clearly not linearly separable (only 55.1% accuracy). The Polynomial kernel with degree 2 does a lot better. RBF has about the same performance as the linear kernel, which means it is very likely underfitting with the default (gamma) parameters. Let's investigate further.\n",
    "\n",
    "Degree | Accuracy\n",
    "--- | ---\n",
    "1 | 64.1283\n",
    "2 | 64.7963\n",
    "3 | 66.1323\n",
    "4 | 68.7375\n",
    "5 | 73.7475\n",
    "10 | 77.01\n",
    "50 | 51.37\n",
    "\n",
    "We can see that performance gradually increases if we increase the degree (use a more complex model). This means that it was previously underfitting. For very large values (50) performance drops sharply, because the model starts overfitting. \n",
    "\n",
    "* Varying gamma yields the following results:\n",
    "\n",
    "Gamma | Accuracy\n",
    "--- | ---\n",
    "0.001 | 55.1102\n",
    "0.01 | 55.1102\n",
    "0.1 | 59.7862\n",
    "1 | 70.4743\n",
    "10 | 83.9011\n",
    "100 | 88.1096\n",
    "1000 | 58.851\n",
    "10000 | 58.7174\n",
    "\n",
    "Increasing gamma leads to more narrow kernels, and hence more complex models. Again, performance increases as we increase gamma, which means that we are underfitting when using values that are too small. The optimal value lies around 100, after which we start overfitting, and performance drops sharply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Neural Networks (2 points)\n",
    "Evaluate MultilayerPerceptrons on the Covertype dataset https://www.openml.org/d/150.\n",
    "\n",
    "- Because this is a large dataset (and MLPs are slow), it is best to take a stratified sumsample of the data. Use the supervised.instance.Resample filter, and take a 0.1% (that's 0.001) subsample.\n",
    "- Build a MultiLayerPerceptron (default settings) using the default settings and report its performance (accuracy).\n",
    "- Observe the outputted weights. For Node 0, which incoming node has the strongest connection with this node (highest absolute weight)?  \n",
    "- Vary the learning rate using values [0.1,0.2,0.3,0.5,1]. Which gives you the best performance? Where do you think the optimal value lies?\n",
    "- Vary the number of hidden layers using values [1,2,3,4,5]. You can keep the default size of the layers (denoted by 'a'). Hence, to build a 5-layer network, you need to set the `hiddenLayers` field to 'a,a,a,a,a'. Which gives you the best performance? Explain why you think this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Solution **\n",
    "\n",
    "* Applying the supervised.instance.Resample filter with sample size 0.1% downsamples the dataset from 581012 instances to 577 instances.\n",
    "* MultiLayerPerceptron yields an accuracy of 67.07%\n",
    "* The strength of the weights varied wildly (after training). The strongest connection with Node 0 is the one with node 29 (weight -10.73). This is a negative value, which means that if the output of node 29 increases, the output of node 0 strongly decreases. \n",
    "* Varying the learning rate yields the following results:\n",
    "\n",
    "Learning rate | Accuracy\n",
    "--- | ---\n",
    "0.1 | 67.9376%\n",
    "0.2 | 67.591%\n",
    "0.3 | 67.0711%\n",
    "0.5 | 66.5511%\n",
    "1 | 63.9515%\n",
    "\n",
    "* The optimal observed value is 0.1, but performance decreases for larger values, which means that the actual optimal value is likely smaller. Indeed, the accuracy for learning rate 0.01 is 71.5771%.\n",
    "* When we vary the number of hidden layers (with fixed hidden layer size), we observe:\n",
    "\n",
    "Number of layers | Accuracy\n",
    "--- | ---\n",
    "1 | 67.0711%\n",
    "2 | 68.1109%\n",
    "3 | 68.9775%\n",
    "4 | 61.3518%\n",
    "5 | 49.0468%\n",
    "\n",
    "The optimal value is 3, after which performance drops significantly. Indeed, the neural net will start overfitting the (limited) training data it has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise 7: OpenML experiments (1 point)\n",
    "Upload your first experiment to OpenML, using task https://www.openml.org/t/3951 (Classification on the Desharnais dataset).\n",
    "\n",
    "- From the WEKA Package Manager, install the OpenmlWeka package.\n",
    "- Read the guidelines for using the graphical interface here: https://www.openml.org/guide#!plugin_weka\n",
    "- Load task 3951, select one (or a few) classifiers, and run them.\n",
    "- On OpenML, find your runs under your profile, and report the run ID so that we can check that everything worked correctly. One run per team is ok,but mention the name of the student that uploaded that run.\n",
    "- Note: by default, OpenML doesn't upload experiments that are identical to earlier experiments. If your experiment does not upload (and no errors are reported), try to run different algorithms (or use different hyperparameters).\n",
    "- Note: for fun, you can compare your performance to other students: https://www.openml.org/t/3951#people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Solution **\n",
    "\n",
    "Any successful run on task 3951 is ok. This is just a preparation for the final assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
